* Deployment happens when there is finished work (that is, no functionality with missing design or vice versa)
* Understand finished work so we can deploy it.
* See the list of tickets being deployed.

Finishing a ticket:

* Developers prioritize tickets from the most recent milestone.
* Developer or morgan finishes ticket (finished = it doesn't need any other work to be complete, they don't need to assign to anyone else to complete it)
* Checks their changes into master when they finish it
* Mark the ticket Resolved Fixed and note the commit in the ticket.
* Push changes to the Dev site.
* Each Night at x hours, all changes on dev site pushed to the staging server
* On staging server, selenium regressions tests are automatically run.
* Tester manually tests fixed bugs and marks as verified.
* Tester checks daily results of selenium test run.

Release:

* Completion of the last ticket in the milestone starts the Release Process.
* Dev pushes last commit to staging server, and sends email to SubtitleDev mailing list
* Start human testing process (email to holmes, janet, testers with change list, blog post)
   * holmes (and maybe janet) processes feedback (from email and user submitted bugs in bugzilla) assigns stuff that meets threshold (should be rare) as bug for that milestone, notifies developers or gives goahead to deploy.
* developers submit fix to same milestone, notify janet or holmes and retest (with possible additional email to humans)
* holmes or janet gives goahead to deploy.
* developer deploys

Testing process:
* Automated regression testing using selenium.  Ideally, developers need to be able to activate and interpret, but discuss with Janet.  
* Automated testing needs to get updated with new features changes... at least, developers, morgan need to give janet notes on changing the test.  A bugzilla search for list of tickets solved since last deploy.
* Possibly get an intern or elancer to do this
* Automated testing runs again after changes made to testing process.  (Janet runs it this time)
* Email to testers and janet with changelog, whatever.  Includes list of changes.  They test new stuff and core functionality to make sure we didn't break anything tragically.
* Look for error messages (any automated messages we get from the widget, from django??  any more we can get that are useful?) feedback form, this should go straight to developers, what's the best way to get this, what's the best threshhold for listening?
* Includes testing workflow document (can we autogenerate based on selenium??) ask janet for a good way to track how many people have gone through testing workflow (visit a page, click a box) and make sure we get a certain number of these.
* we try to get an intern or a hardcore volunteer to maintain selenium tests?

   * Notes for Janet for Testing process
    1. We have automated testing using selenium.  These tests should be located on a pcf server and run nightly  results posted and accessible by all via web.
   2. Tests are updated after bug fixed in bugzilla.  Can use the needs testing flag (or make custom flag) to indicate that a particular test requires a new test generated or a test to be updated.  Each automated test should have a manual test case in Litmus. Results can be entered in litmus for automated test runs too.
  3. Community testing should be a simple suite of manual tests that are tracked in Litmus so that we can see the coverage, os and browsers used.  We get regular interns from Hitek who can be relied upon to run tests when required.
  4. Test results are available in Litmus to show the breakdown of #testers, tests run, browser / os, and pass/fail results.

After deploy: 
* Deploy at the beginning of the day for developer availability if there are problems.
* Look for error messages (any automated messages we get from the widget, from django??  any more we can get that are useful?) feedback form, this should go straight to developers, what's the best way to get this, what's the best threshhold for listening?
* Feedback form should have a release number / timestamp.
* Possibly, do a blog post.


General Notes from Janet:
 * Overall, I think our process among PCF projects should be consistent.   Development for a web project moves faster and deploys more readily, but the general organization and use of bugzilla should the same.  In terms of testing, automated and manual, I think it's ideal to continue to track the testing in Litmus.  Consistency in tools will also make it simpler for interns and other volunteers to contribute to multiple PCF projects.

 * Is the staging area only going to be used for the pre-Deployment testing phase, some short pre-Release time period?
 * In general, what is the target time period for each dev cycle?
